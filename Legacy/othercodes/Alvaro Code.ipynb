{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84ed6d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.GK7GX5KEQ4F6UYO3P26ULGBQYHGQO7J4.gfortran-win_amd64.dll\n",
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\\\FYP\\\\Data\\\\turb3d_64.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f69acd4caa62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[0mFileToOpen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindowsDataPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[0mProcessData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataProcessingClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataPath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFileToOpen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mGridSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDataSize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mReduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mReductionFactor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m \u001b[0mProcessData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCreate_Data_Structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[0mProcessData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLESDiction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[0mProcessData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianCubeCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-f69acd4caa62>\u001b[0m in \u001b[0;36mCreate_Data_Structure\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mraw_dat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFortranFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataPath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGridSize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\io\\_fortran.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, header_dtype)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%sb'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_header_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheader_dtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\\\FYP\\\\Data\\\\turb3d_64.dat'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import struct\n",
    "# import os\n",
    "from pathlib import Path\n",
    "from scipy.io import FortranFile\n",
    "import pickle\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "\n",
    "class DataProcessingClass:\n",
    "\n",
    "    def __init__(self, DataPath, GridSize, Reduction):\n",
    "        self.DataPath = DataPath\n",
    "        self.GridSize = GridSize\n",
    "        self.Reduction = Reduction\n",
    "        self.HaloSize = int(self.GridSize/self.Reduction)\n",
    "        self.HaloNumberTotal = Reduction**3\n",
    "        self.CurrentPos = np.array([0,0,0])\n",
    "        self.VirtualOriginLocal = np.array([0,0,0])\n",
    "        self.VirtualOriginGlobal = np.array([0,0,0])\n",
    "        self.GaussianBox = np.zeros([self.HaloSize,self.HaloSize,self.HaloSize])\n",
    "        self.Cube = {}\n",
    "        self.Parameters = 'u v w'.split()\n",
    "        self.LESDictionary = {}\n",
    "        self.LESPointDictionary = {}\n",
    "\n",
    "    def Create_Data_Structure(self):\n",
    "\n",
    "        \"\"\"Creates Datastructure, Input fortran .Dat file\"\"\"\n",
    "\n",
    "        data={}\n",
    "        raw_dat = FortranFile(self.DataPath,mode='r')\n",
    "        n = self.GridSize\n",
    "\n",
    "        for index, parameter in enumerate(self.Parameters):\n",
    "            data[parameter]=np.zeros((n,n,n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                for k in range(n):\n",
    "                    point_velocities = raw_dat.read_reals(dtype=np.dtype('f4'))\n",
    "                    for index, parameter in enumerate(self.Parameters):\n",
    "                        data[parameter][i][j][k]=point_velocities[index]\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def Plot_Scalar_2D_Data(self, pparams):\n",
    "\n",
    "        \"\"\"Does this and that\"\"\"\n",
    "\n",
    "        AxesFound = np.array([0,0,0])\n",
    "\n",
    "        if pparams[2] == 'x' or pparams[1] == 'x':\n",
    "            AxesFound[0] = 1\n",
    "        if pparams[2] == 'y' or pparams[1] == 'y':\n",
    "            AxesFound[1] = 1\n",
    "        if pparams[2] == 'z' or pparams[1] == 'z':\n",
    "            AxesFound[2] = 1\n",
    "\n",
    "        if sum(AxesFound) == 2:\n",
    "            AxesFound *= self.GridSize\n",
    "            AxesFound[list(AxesFound).index(0)] = pparams[3]\n",
    "            PlotAxes = True\n",
    "        else:\n",
    "            PlotAxes = False\n",
    "            print('Re write the executable 2D plot axes... Something went wrong')\n",
    "\n",
    "        if PlotAxes:\n",
    "\n",
    "            # mappingColorList = list(np.reshape(Data2D,PointNumber**2,1))\n",
    "            plt.figure()\n",
    "            plt.contourf(np.arange(self.GridSize),np.arange(self.GridSize),self.data[pparams[0]][0:AxesFound[0]][0:AxesFound[1]][0:AxesFound[2]][0])\n",
    "            # plt.colorbar()  # mappable = mappingColorList\n",
    "            plt.colorbar().set_label('Velocity in U direction (m/s)')\n",
    "            # plt.title(('2D Representation of {} velocity at ({},{}) plane and height = {}').format(pparams[0],pparams[1],pparams[2],pparams[3]))\n",
    "            plt.xlabel('Point in x')\n",
    "            plt.ylabel('Point in y')\n",
    "            plt.style.use('seaborn-paper')\n",
    "            plt.show()\n",
    "\n",
    "            return 0\n",
    "\n",
    "    def HaloBox(self):\n",
    "        # self.CurrentPos (IFSTATE)\n",
    "        # Select Corner point based on where the current pointer is at...\n",
    "        CornerPoint = self.CurrentPos\n",
    "        for parameter in self.Parameters:\n",
    "            self.Cube[parameter] = np.zeros((self.HaloSize,self.HaloSize,self.HaloSize))\n",
    "\n",
    "        for parameter in self.Parameters:\n",
    "            self.Cube[parameter] = (self.data[parameter]\n",
    "                                  [CornerPoint[0]:CornerPoint[0]+self.HaloSize,\n",
    "                                   CornerPoint[1]:CornerPoint[1]+self.HaloSize,\n",
    "                                   CornerPoint[2]:CornerPoint[2]+self.HaloSize])\n",
    "        return 0\n",
    "\n",
    "    def translate(self,TranslateVector):\n",
    "        self.CurrentPos += TranslateVector*self.HaloSize\n",
    "        return 0\n",
    "\n",
    "    def MVNCubeCreate(self,STDAddtionalMulti):\n",
    "        GaussianBox = np.zeros([self.HaloSize,self.HaloSize,self.HaloSize])\n",
    "        self.VirtualOriginLocal = 0.5*(self.HaloSize - 1)*np.array([1,1,1])\n",
    "        self.VirtualOriginGlobal = self.VirtualOriginLocal + self.CurrentPos\n",
    "\n",
    "        Multi = 0.5*(self.HaloSize - 1)/self.HaloSize\n",
    "        StanDevXYZ = np.std(np.arange(0,self.HaloSize))*Multi*STDAddtionalMulti\n",
    "        MeanVal = self.VirtualOriginLocal\n",
    "\n",
    "        for z in range(0,self.HaloSize):\n",
    "            for y in range(0,self.HaloSize):\n",
    "                for x in range(0,self.HaloSize):\n",
    "                    GaussianBox[x,y,z] = (1/(((2*np.pi)**1.5)*StanDevXYZ**3))*                        np.exp(-(1/(2*StanDevXYZ))*((x-MeanVal)**2+(y-MeanVal)**2+(z-MeanVal)**2))[0]\n",
    "\n",
    "        K_d = np.sum(GaussianBox)**-1\n",
    "        GaussianBox *= K_d  # KEEP COMMENTED FOR TRAINING PURPSES; COMMENT LATER\n",
    "        self.GaussianBox = GaussianBox\n",
    "        return 0\n",
    "\n",
    "    def GaussianCubeCreate(self):\n",
    "        FilterCube = np.zeros([self.HaloSize,self.HaloSize,self.HaloSize])\n",
    "        self.VirtualOriginLocal = 0.5*(self.HaloSize - 1)*np.array([1,1,1])\n",
    "        MeanVal = self.VirtualOriginLocal\n",
    "\n",
    "        for z in range(0,self.HaloSize):\n",
    "            for y in range(0,self.HaloSize):\n",
    "                for x in range(0,self.HaloSize):\n",
    "                    FilterCube[x,y,z] = (6/(np.pi*(self.HaloSize)**2))**1.5*                        np.exp((-6/(self.HaloSize**2))*((x-MeanVal)**2+(y-MeanVal)**2+(z-MeanVal)**2))[0]\n",
    "\n",
    "        K_d = np.sum(FilterCube)**-1\n",
    "        FilterCube *= K_d  # KEEP COMMENTED FOR TRAINING PURPSES; COMMENT LATER\n",
    "        self.GaussianBox = FilterCube\n",
    "        return 0\n",
    "\n",
    "    def Gaussian2DPlot(self,index_y,index_z):\n",
    "        plt.figure()\n",
    "        plt.contourf(np.arange(self.HaloSize),np.arange(self.HaloSize),self.GaussianBox[:,:,index_z])\n",
    "        plt.colorbar().set_label('Weighting')\n",
    "        plt.xlabel('Point in x')\n",
    "        plt.ylabel('Point in y')\n",
    "        plt.style.use('seaborn-paper')\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "        plt.plot(self.GaussianBox[:,index_y,index_z])\n",
    "        plt.xlabel('Point in x at y = {} and z = {}'.format(index_y,index_z))\n",
    "        plt.ylabel('Weighting')\n",
    "        plt.show()\n",
    "        return 0\n",
    "\n",
    "    def Coarsening_Plot(self, plus, param):\n",
    "        LES_vals = self.LESDictionary['MVV'][param][0][(plus)*self.Reduction**2:(plus+1)*self.Reduction**2]                .reshape(self.Reduction,self.Reduction)\n",
    "        DNS_val_1 = self.data[param][:,:,plus + 1]\n",
    "        DNS_val_2 = self.data[param][:,:,plus+ 2]\n",
    "        DNS_val = 0.5*(DNS_val_1+DNS_val_2)\n",
    "\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.contourf(np.arange(self.GridSize),np.arange(self.GridSize),DNS_val,cmap = 'jet')\n",
    "        plt.xlabel('Point in x')\n",
    "        plt.ylabel('Point in y')\n",
    "        plt.colorbar().set_label('Velocity')\n",
    "\n",
    "        #sns.heatmap(DNS_val,cmap='jet')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.contourf(np.arange(self.Reduction),np.arange(self.Reduction),LES_vals, cmap = 'jet')\n",
    "        plt.colorbar().set_label('Mean Velocity')\n",
    "        plt.xlabel('Point in x')\n",
    "        plt.ylabel('Point in y')\n",
    "        #sns.heatmap(LES_vals,cmap='jet')\n",
    "        return 0\n",
    "\n",
    "    def Coarsen_Plot_1D(self, plusy, param):\n",
    "        LES_vals = self.LESDictionary['MVV'][param][0][(plusy)*self.Reduction:(plusy+1)*self.Reduction]\n",
    "        DNS_vals = self.data[param][:,plusy,0]\n",
    "        plt.plot(np.arange(0,self.GridSize,self.HaloSize),LES_vals, linewidth = 3, c = 'r', label = 'LES')\n",
    "        plt.plot(np.arange(self.GridSize), DNS_vals, linewidth = 1.5, c = 'b', label = 'DNS')\n",
    "        plt.xlabel('Point in X')\n",
    "        plt.ylabel('Velocity (m/s)')\n",
    "        plt.xlim([0,self.GridSize-self.HaloSize])\n",
    "        plt.legend()\n",
    "        return 0\n",
    "\n",
    "    def LESDiction(self):\n",
    "        LESSize = self.HaloNumberTotal\n",
    "        DicMVV = {}\n",
    "        DicMVG = {}\n",
    "        DicIMV = {}\n",
    "        DicMVI = {}\n",
    "        cartesians = \"x y z\".split()\n",
    "\n",
    "        for indexp, parameter in enumerate(self.Parameters):\n",
    "            DicMVV[parameter] = np.zeros([1,LESSize])\n",
    "            for indexc, cartesian in enumerate(cartesians):\n",
    "                DicMVG[parameter+cartesian] = np.zeros([1,LESSize])\n",
    "\n",
    "        for index1, parameter1 in enumerate(self.Parameters):\n",
    "            for index2, parameter2 in enumerate(self.Parameters):\n",
    "                DicIMV[parameter1+parameter2] = np.zeros([1,LESSize])\n",
    "                DicMVI[parameter1+parameter2] = np.zeros([1,LESSize])\n",
    "\n",
    "        self.LESPointDictionary = {\n",
    "            \"PN\":np.zeros([1,LESSize]),\n",
    "            \"PV\":np.zeros([1,3,LESSize]),\n",
    "        }\n",
    "\n",
    "        self.LESDictionary = {\n",
    "            \"MVV\":DicMVV,\n",
    "            \"MVG\":DicMVG,\n",
    "            \"IMV\":DicIMV,\n",
    "            \"MVI\":DicMVI\n",
    "        }\n",
    "        return 0\n",
    "\n",
    "    def AppendPointLocation(self, PointNumb):\n",
    "        self.LESPointDictionary['PN'][0,PointNumb] = PointNumb\n",
    "        for index, params in enumerate(self.Parameters):\n",
    "            self.LESPointDictionary['PV'][0,index,PointNumb] = self.CurrentPos[index]\n",
    "        return 0\n",
    "\n",
    "    def AppendHaloAverage(self, PointNumb, bGaussianFilter):\n",
    "        if bGaussianFilter:\n",
    "            for index, params in enumerate(self.Parameters):\n",
    "                self.LESDictionary['MVV'][params][0,PointNumb] =                    np.tensordot(self.Cube[self.Parameters[index]],self.GaussianBox,axes=3)\n",
    "        else:\n",
    "            for index, params in enumerate(self.Parameters):\n",
    "                self.LESDictionary['MVV'][params][0,PointNumb] =                    np.mean(self.Cube[self.Parameters[index]])\n",
    "        return 0\n",
    "\n",
    "    def AppendMVG(self, PointNumb, bGaussianFilter):\n",
    "        cartesians = \"x y z\".split()\n",
    "        if bGaussianFilter:\n",
    "            for indexp, param in enumerate(self.Parameters):\n",
    "                for indexc, cartesian in enumerate(cartesians):\n",
    "                    self.LESDictionary['MVG'][param+cartesian][0, PointNumb] =                        np.tensordot(np.gradient(self.Cube[param],axis=indexc),self.GaussianBox,axes=3)\n",
    "        else:\n",
    "                    self.LESDictionary['MVG'][param+cartesian][0, PointNumb] =                        np.mean(np.gradient(self.Cube[param],axis=indexc))\n",
    "        return 0\n",
    "\n",
    "    def AppendIMV(self, PointNumb):\n",
    "        for paramcol in self.Parameters:\n",
    "            for paramrow in self.Parameters:\n",
    "                self.LESDictionary['IMV'][paramrow + paramcol][0,PointNumb] =                    self.LESDictionary['MVV'][paramrow][0,PointNumb]*                    self.LESDictionary['MVV'][paramcol][0,PointNumb]\n",
    "        return 0\n",
    "\n",
    "    def AppendMVI(self, PointNumb, bGaussianFilter):\n",
    "        for parami in self.Parameters:\n",
    "            for paramj in self.Parameters:\n",
    "                if bGaussianFilter:\n",
    "                    self.LESDictionary['MVI'][parami + paramj][0,PointNumb] =                        np.tensordot(self.Cube[parami]*self.Cube[paramj],self.GaussianBox,axes=3)\n",
    "                else:\n",
    "                    self.LESDictionary['MVI'][parami + paramj][0,PointNumb] =                        np.mean(self.Cube[parami]*self.Cube[paramj])\n",
    "        return 0\n",
    "\n",
    "    def Coarsen(self):\n",
    "        self.CurrentPos = np.array([0,0,0])\n",
    "        TransVector = np.array([0,0,0])\n",
    "        Coarsened = False\n",
    "        PointNumber = 0\n",
    "\n",
    "        for ziter in range(0,self.Reduction):\n",
    "            for yiter in range(0,self.Reduction):\n",
    "                for xiter in range(0,self.Reduction):\n",
    "\n",
    "                    # Create a cube at the current position (self.CurrentPos)\n",
    "                    self.HaloBox()\n",
    "\n",
    "                    # Append Point location and Position\n",
    "                    self.AppendPointLocation(PointNumber)\n",
    "\n",
    "                    # Append Average velocity\n",
    "                    self.AppendHaloAverage(PointNumber, True)  # Average Velocity vector at virtual point\n",
    "\n",
    "                    # Append mean velocity gradient (MVG)\n",
    "                    self.AppendMVG(PointNumber, True)\n",
    "\n",
    "                    # Append interaction of mean velocities (IMV = mean(ui)*mean(uj))\n",
    "                    self.AppendIMV(PointNumber)\n",
    "\n",
    "                    # Append mean velocity interactions (MVI = mean(ui*uj))\n",
    "                    self.AppendMVI(PointNumber, True)\n",
    "\n",
    "                    # External gradients are done post-coarsening\n",
    "\n",
    "                    if xiter == self.Reduction - 1:\n",
    "                        if yiter != self.Reduction - 1:\n",
    "                            TransVector = np.array([-(self.Reduction-1),1,0])\n",
    "                        elif yiter == self.Reduction - 1 and ziter != self.Reduction - 1:\n",
    "                            TransVector = np.array([-(self.Reduction-1),-(self.Reduction-1),1])\n",
    "                        else:\n",
    "                            print('Coarsening Complete')\n",
    "                            Coarsened = True\n",
    "                            TransVector = np.array([0,0,0])\n",
    "                    else:\n",
    "                        TransVector = np.array([1,0,0])\n",
    "                    self.translate(TransVector)\n",
    "                    PointNumber += 1\n",
    "        return Coarsened\n",
    "\n",
    "    def CreateDataFrame(self, detail = False):\n",
    "        DatFrameLES = pd.DataFrame.from_dict({(i,j): self.LESDictionary[i][j][0]\n",
    "             for i in self.LESDictionary.keys()\n",
    "             for j in self.LESDictionary[i].keys()})\n",
    "\n",
    "        LESDicPoint = pd.DataFrame.from_dict({('PV',jpar): self.LESPointDictionary['PV'][0,jind]\n",
    "                for jind, jpar in enumerate('x y z'.split())})\n",
    "\n",
    "        FinalFrame = pd.concat([LESDicPoint, DatFrameLES],axis=1)\n",
    "\n",
    "        stringTurb = 'uu uv uw vv vw ww'.split()\n",
    "        for turbparam in stringTurb:\n",
    "            FinalFrame['RS_' + turbparam] = FinalFrame['MVI'][turbparam] - FinalFrame['IMV'][turbparam]\n",
    "        if detail is False:\n",
    "            FinalFrame.drop(['PV','IMV','MVI'],axis=1,inplace=True)\n",
    "        return FinalFrame\n",
    "\n",
    "    def save_obj(self,name):\n",
    "        with open(\"C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\FYP\\\\Diction\\\\\" + name + '.pkl', 'wb') as f:\n",
    "            pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load_obj(name):\n",
    "        with open(\"C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\FYP\\\\Diction\\\\\" + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "DataSetMult = 1\n",
    "DataSize = 64*DataSetMult\n",
    "ReductionFactor = 16*DataSetMult\n",
    "windowsDataPath = Path(\"C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\\\FYP\\\\Data\\\\turb3d_\" + str(DataSize) + \".dat\")\n",
    "FileToOpen = Path(windowsDataPath)\n",
    "ProcessData = DataProcessingClass(DataPath=FileToOpen,GridSize=DataSize,Reduction=ReductionFactor)\n",
    "ProcessData.Create_Data_Structure()\n",
    "ProcessData.LESDiction()\n",
    "ProcessData.GaussianCubeCreate()\n",
    "ProcessData.Coarsen()\n",
    "DataFrame = ProcessData.CreateDataFrame(False)\n",
    "# ProcessData.save_obj(\"LESDict_{}_{}\".format(ProcessData.GridSize,ProcessData.HaloSize))\n",
    "# DataFrame.to_csv(path_or_buf=\"C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\\\FYP\\\\Data\\\\DFrame128_2.csv\",encoding=\"utf-8\")\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(\"C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\FYP\\\\Diction\\\\\" + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "# ProcessData = load_obj(\"LESDict_128_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad22e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1110]:\n",
    "\n",
    "\n",
    "# save_obj(ProcessData,\"LESDict_{}_{}\".format(DataSize,ProcessData.HaloSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9025e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().run_line_magic('matplotlib', 'qt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c46ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame.to_csv(path_or_buf=\"C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\\\FYP\\\\Data\\\\DFrame256.csv\",encoding=\"utf-8\")\n",
    "## When Reading Data From Excel...\n",
    "# DatUP = pd.read_csv(\"C:\\\\Users\\\\Alvaro\\\\Desktop\\\\Alvaro\\\\Imperial College\\\\ME4\\\\FYP\\\\Data\\\\Dframe256.csv\",encoding=\"utf-8\",delimiter=';')\n",
    "# DataFrame = DatUP['u v w ux uy uz vx vy vz wx wy wz uu uv uw vv vw ww'.split()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e96e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessData = load_obj(\"LESDict_256_4\")\n",
    "DataFrame = ProcessData.CreateDataFrame(False)\n",
    "DataFrame.columns = DataFrame.columns.droplevel()\n",
    "DataFrame.columns = ['u', 'v', 'w', 'ux', 'uy', 'uz', 'vx', 'vy', 'vz', 'wx', 'wy', 'wz', 'uu',\n",
    "      'uv', 'uw', 'vv', 'vw', 'ww']\n",
    "DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aed85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 2048\n",
    "n_inputs = 3 + 9\n",
    "n_hidden_1 = 24\n",
    "n_hidden_2 = 12\n",
    "n_hidden_3 = 24\n",
    "n_data_points = (ProcessData.Reduction)**3\n",
    "n_classes = 6\n",
    "frac = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cbafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestTrainExtract(DataFrame,frac):\n",
    "    TrainingDF = DataFrame.sample(frac=frac,random_state=1)\n",
    "    TestDF = DataFrame.drop(index = TrainingDF.index.values,axis = 1)\n",
    "    return TrainingDF, TestDF\n",
    "\n",
    "def norm(x):\n",
    "    return (x - TrainStats['mean']) / TrainStats['std']\n",
    "\n",
    "def SplitTestEval(TrainFrame):\n",
    "    test_x, test_y = np.split(TrainFrame,[n_inputs],axis=1)\n",
    "    return test_x, test_y\n",
    "\n",
    "def MLPerceptron(x, weights, biases):\n",
    "\n",
    "    '''\n",
    "    x: Placeholder for Data Input\n",
    "    Weights : Dicts of Weights\n",
    "    Biases: Dicts of Biases\n",
    "    '''\n",
    "\n",
    "    # First Layer\n",
    "    layer_1 = tf.add(tf.matmul(x,weights['w1']),biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "\n",
    "    # Second layer\n",
    "    layer_2 = tf.add(tf.matmul(layer_1,weights['w2']),biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "    # Third Layer\n",
    "    layer_3 = tf.add(tf.matmul(layer_2,weights['w3']),biases['b3'])\n",
    "    layer_3 = tf.nn.sigmoid(layer_3)\n",
    "\n",
    "    # Output Layer\n",
    "    OutputLayer = tf.matmul(layer_3,weights['out']) + biases['out']\n",
    "\n",
    "    return OutputLayer\n",
    "\n",
    "def NextBatch(BatchSize, BatchIter, RemainingData):\n",
    "    Batch = RemainingData.sample(n = BatchSize,random_state=1)\n",
    "    RemainingData.drop(index = Batch.index.values,axis = 1, inplace = True)\n",
    "    Batch_x, Batch_y = np.split(Batch,[n_inputs],axis=1)\n",
    "    return Batch_x, Batch_y\n",
    "\n",
    "def NextBatchNormalised(BatchSize, BatchIter, RemainingData):\n",
    "    Batch = RemainingData.sample(n = BatchSize,random_state=1)\n",
    "    RemainingData.drop(index = Batch.index.values,axis = 1, inplace = True)\n",
    "    return Batch\n",
    "\n",
    "#MAKE THE TRAIN AND TEST DATA\n",
    "Train, Test = TestTrainExtract(DataFrame, frac)\n",
    "\n",
    "test_x, test_y = SplitTestEval(Test)\n",
    "train_x, train_y = SplitTestEval(Train)\n",
    "\n",
    "TrainStats = train_x.describe().transpose()\n",
    "\n",
    "NormedTest_x = norm(test_x)\n",
    "NormedTrain_x = norm(train_x)\n",
    "\n",
    "# sns.pairplot(Train[[\"u\", \"v\",\"w\"]], diag_kind=\"kde\")\n",
    "\n",
    "#CREATE THE WEIGHTS AND BIASES DICTIONARIES\n",
    "weights = {\n",
    "    'w1':tf.Variable(tf.random_normal([n_inputs,n_hidden_1])),\n",
    "    'w2':tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "    'w3':tf.Variable(tf.random_normal([n_hidden_2,n_hidden_3])),\n",
    "    'out':tf.Variable(tf.random_normal([n_hidden_3,n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1':tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2':tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3':tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out':tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "x = tf.placeholder(tf.float32,shape = [None,n_inputs])\n",
    "y = tf.placeholder(tf.float32,shape = [None,n_classes])\n",
    "\n",
    "#ASSIGN THE OPISMISIERS AND COST FUNCTION\n",
    "Prediction = MLPerceptron(x, weights, biases)\n",
    "Cost = tf.reduce_mean(tf.square(Prediction - y))\n",
    "Optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(Cost)\n",
    "#Optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(Cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed71f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    "tf.InteractiveSession.close(sess)\n",
    "sess = tf.InteractiveSession()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "avg_cost_old = 100000\n",
    "cost_arr = []\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_cost = 0.0\n",
    "    total_batch_number = int(n_data_points*frac/batch_size)\n",
    "    RemainingData_x = NormedTrain_x.copy()\n",
    "    RemainingData_y = train_y.copy()\n",
    "\n",
    "    for BatchIter in range (total_batch_number):\n",
    "        Batch_x = NextBatchNormalised(batch_size, BatchIter, RemainingData_x)\n",
    "        Batch_y = NextBatchNormalised(batch_size, BatchIter, RemainingData_y)\n",
    "        _,c = sess.run([Optimizer,Cost],feed_dict={x:Batch_x,y:Batch_y})\n",
    "        avg_cost += c/total_batch_number\n",
    "\n",
    "    print(\"Epoch: {}, cost: {}\".format(epoch+1, avg_cost))\n",
    "    cost_arr.append(avg_cost)\n",
    "    if (avg_cost_old > avg_cost):\n",
    "        avg_cost_old = avg_cost\n",
    "    else:\n",
    "        break\n",
    "\n",
    "print(\"Complete Model {} Epochs of training\".format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f574191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,epochs+1),cost_arr,label='Cost',linewidth=2)\n",
    "plt.xlim([1,epochs])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = Prediction.eval(feed_dict={x:NormedTest_x})\n",
    "(preds-test_y).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be492c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.rcParams.update({'font.size':10})\n",
    "summ_dict = {}\n",
    "for index, param in enumerate('uu uv uw vv vw ww'.split()):\n",
    "    summ_dict[param] = np.corrcoef(test_y[param],preds[:,index])[0,1]\n",
    "    plt.subplot(2,3,index + 1)\n",
    "    sns.scatterplot(test_y[param],preds[:,index],label=\"SGS ({}), Corr. {:.2f}\".format(param,summ_dict[param]))\n",
    "    plt.legend()\n",
    "    if index > 2:\n",
    "        plt.xlabel('Test Value',)\n",
    "    if index == 0 or index == 3:\n",
    "        plt.ylabel('Predicted Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9fe15",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame.from_dict(data=summ_dict,orient='index')\n",
    "summary.columns = ['Correlation']\n",
    "summary['Correlation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841aac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(test_y['ww'],preds[:,5])\n",
    "plt.plot(np.arange(0,60),np.arange(0,60),'r')\n",
    "err = (test_y['vv'] - preds[:,3])\n",
    "sns.kdeplot(preds[:,0],test_y['uu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.InteractiveSession.close(sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
